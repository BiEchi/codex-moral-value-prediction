{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../../data/moral/interim/train_test/twitter/train_bert_binary.json'\n",
    "test_path = '../../../data/moral/interim/train_test/twitter/test_bert_binary.json'\n",
    "output_path = '../../../data/moral/processed/twitter/prompt/baseline-1ex/'\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(test_path, lines=True)\n",
    "\n",
    "train_data_1 = train_data[train_data['label'] == 1]\n",
    "train_data_0 = train_data[train_data['label'] == 0]\n",
    "\n",
    "prompts_df = pd.DataFrame(columns=['line_idx', 'original_example', 'full_prompt', 'instantiation_prompt'])\n",
    "\n",
    "for idx, test_sample in test_data.iterrows():\n",
    "    # randomly sample 1 example from train with label 1 and 1 example from train with label 0\n",
    "    eg1 = train_data_1.sample(1)['text'].values[0]\n",
    "    res1 = 1\n",
    "    eg2 = train_data_0.sample(1)['text'].values[0]\n",
    "    res2 = 0\n",
    "    infer = test_sample['text']\n",
    "\n",
    "    # design prompts for inferring codex\n",
    "    prompt = f\"\"\"determine whether the following sentence presents a moral value or not. Return 1 if it does, 0 if it does not. Don't return anything else.\n",
    "    Example with moral value: \n",
    "    Sentence: {eg1.strip()} \n",
    "    Return: {res1}\n",
    "    Example without moral value:\n",
    "    Sentence: {eg2.strip()}\n",
    "    Return: {res2}\n",
    "    Do Inference:\n",
    "    Sentence: {infer.strip()}\n",
    "    Return: \"\"\"\n",
    "    \n",
    "    prompts_df = prompts_df.append({'line_idx': idx, 'original_example': test_sample.to_dict(), 'full_prompt': prompt, 'instantiation_prompt': ''}, ignore_index=True)\n",
    "\n",
    "    # write prompt to file\n",
    "    if not os.path.exists(output_path + 'test/'):\n",
    "        os.makedirs(output_path + 'test/')\n",
    "    with open(output_path + 'test/' + str(idx) + '.txt', 'w') as f:\n",
    "        f.write(prompt)\n",
    "        \n",
    "# save the prompts df\n",
    "prompts_df.to_json(output_path + 'test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = '../../../data/moral/interim/train_test/reddit/train_bert_binary.json'\n",
    "test_path = '../../../data/moral/interim/train_test/reddit/test_bert_binary.json'\n",
    "output_path = '../../../data/moral/processed/reddit/prompt/baseline-1ex/'\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(test_path, lines=True)\n",
    "\n",
    "train_data_1 = train_data[train_data['label'] == 1]\n",
    "train_data_0 = train_data[train_data['label'] == 0]\n",
    "\n",
    "prompts_df = pd.DataFrame(columns=['line_idx', 'original_example', 'full_prompt', 'instantiation_prompt'])\n",
    "\n",
    "for idx, test_sample in test_data.iterrows():\n",
    "    # randomly sample 1 example from train with label 1 and 1 example from train with label 0\n",
    "    eg1 = train_data_1.sample(1)['text'].values[0]\n",
    "    res1 = 1\n",
    "    eg2 = train_data_0.sample(1)['text'].values[0]\n",
    "    res2 = 0\n",
    "    infer = test_sample['text']\n",
    "\n",
    "    # design prompts for inferring codex\n",
    "    prompt = f\"\"\"determine whether the following sentence presents a moral value or not. Return 1 if it does, 0 if it does not. Don't return anything else.\n",
    "    Example with moral value: \n",
    "    Sentence: {eg1.strip()} \n",
    "    Return: {res1}\n",
    "    Example without moral value:\n",
    "    Sentence: {eg2.strip()}\n",
    "    Return: {res2}\n",
    "    Do Inference:\n",
    "    Sentence: {infer.strip()}\n",
    "    Return: \"\"\"\n",
    "    \n",
    "    prompts_df = prompts_df.append({'line_idx': idx, 'original_example': test_sample.to_dict(), 'full_prompt': prompt, 'instantiation_prompt': ''}, ignore_index=True)\n",
    "\n",
    "    # write prompt to file\n",
    "    if not os.path.exists(output_path + 'test/'):\n",
    "        os.makedirs(output_path + 'test/')\n",
    "    with open(output_path + 'test/' + str(idx) + '.txt', 'w') as f:\n",
    "        f.write(prompt)\n",
    "        \n",
    "# save the prompts df\n",
    "prompts_df.to_json(output_path + 'test.jsonl', orient='records', lines=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Multilabel classification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5524it [03:22, 27.29it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../../data/moral/interim/train_test/twitter/train_bert.json'\n",
    "test_path = '../../../data/moral/interim/train_test/twitter/test_bert.json'\n",
    "output_path = '../../../data/moral/processed/twitter/prompt/baseline-50exwxp/'\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(test_path, lines=True)\n",
    "\n",
    "moral_values = ['care_or_harm', 'fairness_or_cheating', 'loyalty_or_betrayal', 'authority_or_subversion', 'purity_or_degradation', 'non-moral']\n",
    "\n",
    "num_examples = 50\n",
    "prompts_df = pd.DataFrame(columns=['line_idx', 'original_example', 'full_prompt', 'instantiation_prompt'])\n",
    "\n",
    "for idx, test_sample in tqdm(test_data.iterrows()):\n",
    "\n",
    "    prompt = f\"\"\"There are 5 possible moral values present in a sentence, but it's also possible that no moral values are present at all.\n",
    "If any of the moral values are present, return the list of moral values present in the sentence. If no moral values are present, return ['non-moral'].\n",
    "The total set of moral values are: {str(moral_values)}\n",
    "care_or_harm is related to our long evolution as mammals with attachment systems and an ability to feel (and dislike) the pain of others. It underlies virtues of kindness, gentleness, and nurturance.\n",
    "fairness_or_cheating is related to the evolutionary process of reciprocal altruism. It generates ideas of justice, rights, and autonomy.\n",
    "loyalty_or_betrayal is related to our long history as tribal creatures able to form shifting coalitions. It underlies virtues of patriotism and self-sacrifice for the group. It is active anytime people feel that it’s “one for all, and all for one.”\n",
    "authority_or_subversion is related to our long primate history of hierarchical social interactions. It underlies virtues of leadership and followership including deference to legitimate authority and respect for traditions.\n",
    "purity_or_degradation is shaped by the psychology of disgust and contamination. It underlies religious notions of striving to live in an elevated, less carnal, more noble way. It underlies the widespread idea that the body is a temple that can be desecrated by immoral activities and contaminants (an idea not unique to religious traditions).\n",
    "Examples:\\n\"\"\"\n",
    "    \n",
    "    # for i in range(num_examples * len(moral_values)):\n",
    "    #     ex = train_data[train_data[moral_values[i % len(moral_values)]] == 1].sample(1)\n",
    "    #     present_moral_values = ex.columns[ex.eq(1).any()].tolist()\n",
    "    #     prompt_slice_i = f\"\"\"\\nSentence: {ex['text'].values[0].strip()}\\nReturn: {str(present_moral_values)}\\n\"\"\"\n",
    "    #     prompt += prompt_slice_i\n",
    "        \n",
    "    for i in range(num_examples):\n",
    "        ex = train_data.sample(1)\n",
    "        present_moral_values = ex.columns[ex.eq(1).any()].tolist()\n",
    "        prompt_slice_i = f\"\"\"\\nSentence: {ex['text'].values[0].strip()}\\nReturn: {str(present_moral_values)}\\n\"\"\"\n",
    "        prompt += prompt_slice_i\n",
    "        \n",
    "    prompt += f\"\"\"\\nPredict:\\nSentence: {test_sample['text'].strip()}\\nReturn: \"\"\"\n",
    "\n",
    "    prompts_df = prompts_df.append({'line_idx': idx, 'original_example': test_sample.to_dict(), 'full_prompt': prompt, 'instantiation_prompt': ''}, ignore_index=True)\n",
    "\n",
    "    # write prompt to file\n",
    "    if not os.path.exists(output_path + 'test/'):\n",
    "        os.makedirs(output_path + 'test/')\n",
    "    with open(output_path + 'test/' + str(idx) + '.txt', 'w') as f:\n",
    "        f.write(prompt)\n",
    "         \n",
    "prompts_df.to_json(output_path + 'test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2689it [00:19, 136.00it/s]\n"
     ]
    }
   ],
   "source": [
    "train_path = '../../../data/moral/interim/train_test/reddit/train_bert.json'\n",
    "test_path = '../../../data/moral/interim/train_test/reddit/test_bert.json'\n",
    "output_path = '../../../data/moral/processed/reddit/prompt/baseline-1expmv/'\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "train_data = pd.read_json(train_path, lines=True)\n",
    "test_data = pd.read_json(test_path, lines=True)\n",
    "\n",
    "moral_values = ['Purity', 'Loyalty', 'Non-Moral', 'Authority', 'Equality', 'Care', 'Proportionality']\n",
    "\n",
    "num_examples = 1\n",
    "prompts_df = pd.DataFrame(columns=['line_idx', 'original_example', 'full_prompt', 'instantiation_prompt'])\n",
    "\n",
    "for idx, test_sample in tqdm(test_data.iterrows()):\n",
    "    prompt = f\"\"\"There are 6 possible moral values present in a sentence, but it's also possible that no moral values are present at all.\n",
    "    If any of the moral values are present, return the list of moral values present in the sentence. If no moral values are present, return ['Non-Moral'].\n",
    "    The total set of moral values are: {str(moral_values)}\\n\"\"\"\n",
    "\n",
    "    for i in range(num_examples * len(moral_values)):\n",
    "        ex = train_data[train_data[moral_values[i]] == 1].sample(1)\n",
    "        present_moral_values = ex.columns[ex.eq(1).any()].tolist()\n",
    "        prompt_slice_i = f\"\"\"Example with {moral_values[i]} present:\\nSentence: {ex['text'].values[0].strip()}\\nReturn: {str(present_moral_values)}\\n\"\"\"\n",
    "        prompt += prompt_slice_i\n",
    "        \n",
    "    prompt += f\"\"\"Predict:\\nSentence: {test_sample['text'].strip()}\\nReturn: \"\"\"\n",
    "\n",
    "    prompts_df = prompts_df.append({'line_idx': idx, 'original_example': test_sample.to_dict(), 'full_prompt': prompt, 'instantiation_prompt': ''}, ignore_index=True)\n",
    "\n",
    "    # write prompt to file\n",
    "    if not os.path.exists(output_path + 'test/'):\n",
    "        os.makedirs(output_path + 'test/')\n",
    "    with open(output_path + 'test/' + str(idx) + '.txt', 'w') as f:\n",
    "        f.write(prompt)\n",
    "        \n",
    "prompts_df.to_json(output_path + 'test.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38_default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c975a20906e8142c699e3b310c99f06555776b58f1706bf1df8dfa61229c3f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
